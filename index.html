<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Face Tracking with 3D Model</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face-detection/face-detection.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera-utils/camera_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing-utils/drawing_utils.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.150.0/build/three.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/three@0.150.0/examples/js/loaders/GLTFLoader.js"></script>
    <style>
      body { margin: 0; overflow: hidden; }
      canvas, video { position: absolute; top: 0; left: 0; }
    </style>
  </head>
  <body>
    <video id="video" autoplay muted playsinline style="display:none;"></video>
    <script>
      let video = document.getElementById('video');

      // Setup Three.js scene
      const scene = new THREE.Scene();
      const camera = new THREE.PerspectiveCamera(70, window.innerWidth / window.innerHeight, 0.01, 10);
      camera.position.z = 2;

      const renderer = new THREE.WebGLRenderer({ antialias: true, alpha: true });
      renderer.setSize(window.innerWidth, window.innerHeight);
      document.body.appendChild(renderer.domElement);

      // Load GLB Model
      const loader = new THREE.GLTFLoader();
      let model;
      loader.load('Open_Door.glb', function(gltf) {
        model = gltf.scene;
        model.scale.set(0.5, 0.5, 0.5);
        scene.add(model);
      });

      // Setup MediaPipe face detection
      const faceDetection = new FaceDetection.FaceDetection({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face-detection/${file}` });
      faceDetection.setOptions({ model: 'short', minDetectionConfidence: 0.5 });
      faceDetection.onResults(onResults);

      async function onResults(results) {
        if (results.detections.length > 0 && model) {
          const face = results.detections[0].boundingBox;
          const x = (face.xCenter / video.videoWidth) * 2 - 1;
          const y = -((face.yCenter / video.videoHeight) * 2 - 1);
          model.position.set(x, y, 0);
        }
        renderer.render(scene, camera);
      }

      const cameraUtils = new CameraUtils.Camera(video);
      cameraUtils.start();
      async function detectLoop() {
        await faceDetection.send({ image: video });
        requestAnimationFrame(detectLoop);
      }
      detectLoop();
    </script>
  </body>
</html>
